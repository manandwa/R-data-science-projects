---
title: "Guided Project: Winning Jeopardy"
author: "Mobin Anandawla"
date: "July 8, 2020"
output: html_notebook
---

# Introduction

This dataset is a database of 200000 rows.  we will look at this dataset to see if there are patterns we can use to help us prepare for Jeopardy.  The dataset was obtained from the r/datasets subreddit and can be downloaded [here](https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/) using the google drive link for best results

We will load and examine a few rows of the dataset
```{r}
library(tidyverse)

jeopardy <- read_csv("JEOPARDY_CSV.csv")
head(jeopardy, 5)
```

Also we will get the column names
```{r}
colnames(jeopardy)
```

```{r}
colnames(jeopardy) <- c("show_number", "air_date", "round", "category", "value", "question", "answer")
```

We will also get the type of each column
```{r}
sapply(jeopardy, typeof)
```

The following columns are doubles: `show_number`, and `air_date`, the rest are of type `character`

# Fixing Data Types

Let's look at the unique values in the dataset
```{r}
unique(jeopardy$value)
```

Here is what we need to do with this column


* Convert to numeric values
* Remove all `None` Values (missing data)
* Remove all `$` and `,`

```{r}
jeopardy <- jeopardy %>% filter(value != "None") %>% mutate(value = str_replace_all(value, "[$,]",""), value = as.numeric(value))
```

Rechcking value
```{r}
typeof(jeopardy$value)
```

```{r}
unique(jeopardy$value)
```

# Normalizing Text

We will now focus on the following columns, `question`, `answer` and `category` and perform normalization doing the following

* making all the text lowercase
* Remove all punctuation keeping only letters and numbers

```{r}
head(unique(jeopardy$question), 10)
```

```{r}
head(unique(jeopardy$answer), 10)
```

```{r}
head(unique(jeopardy$category), 10)
```

These are the columns in their raw form we will now clean them
```{r}
jeopardy <- jeopardy %>% mutate(question = tolower(question), question = str_replace_all(question, "[^A-Za-z0-9 ]", ""), answer = tolower(answer), answer = str_replace_all(answer, "[^A-Za-z0-9 ]", ""), category = tolower(category), category = str_replace_all(category, "[^A-Za-z0-9 ]", ""))
```

Here are the columns after normalization

```{r}
head(unique(jeopardy$question), 10)
```

```{r}
head(unique(jeopardy$answer), 10)
```

```{r}
head(unique(jeopardy$category), 10)
```

Let's look at the dataset now
```{r}
head(jeopardy, 10)
```

# Making Dates More Accessible

We will now look at the `air_date` column and split it day, month and year values using the `separate()` function and convert these into numeric values

```{r}
jeopardy <- jeopardy %>% separate(., air_date, into = c("year", "month", "day"), sep = "-") %>% mutate(year = as.numeric(year), month = as.numeric(month), day = as.numeric(day))
```

Checking the dataset with the new columns
```{r}
head(jeopardy, 10)
```

# Focusing on Particular Subject Areas

Now that we have a cleaned up dataset we can focus on asking questions based on the dataset and forming hypothesises from it.  One question is that what categories come up in Jeopardy.  Some say it is science and history where as others say it is Shakesphere (Ken Jennings would know better though on that).

We have some set probabilities that we are using.  There are 3369 unique categories and there the probability of an expected category is $\frac{1}{3369}$ and the probability of not getting a an expected category is $\frac{3368}{3369}$

```{r}
n_questions <- nrow(jeopardy)
p_category_expected <- 1/3369
p_not_category_expected <- 3368/3369
p_expected <- c(p_category_expected, p_not_category_expected)

# Checking how many times science shows up in the category column
categories <- pull(jeopardy, category)

n_science_category <- 0
for (cat in categories) {
  if ("science" %in% cat) {
    n_science_category <- n_science_category + 1
  } 
}

science_observed <- c(n_science_category, (n_questions - n_science_category))
chisq.test(science_observed, p = p_expected)
```

```{r}
n_history_category <- 0
for (cat in categories) {
  if ("history" %in% cat) {
    n_history_category <- n_history_category + 1
  } 
}

history_observed <- c(n_history_category, (n_questions - n_history_category))
chisq.test(history_observed, p = p_expected)
```

```{r}
n_shakespeare_category <- 0
for (cat in categories) {
  if ("shakespeare" %in% cat) {
    n_shakespeare_category <- n_shakespeare_category + 1
  } 
}

shakespeare_observed <- c(n_shakespeare_category, (n_questions - n_shakespeare_category))
chisq.test(shakespeare_observed, p = p_expected)
```

Since the p-values are all less then 0.05 we reject the null-hypothesis (science doesn't have a higher presence in jeopardy questions and the same conclusion is reached for history and Shakespeare)

# Unique Terms in Questions

We will now investigate whether new questions are repeats of old ones we are using the whole jeopardy dataset

```{r}
# empty vector to store terms
terms_used <- character(0)

# Get the questions
questions <- pull(jeopardy, question)

for (quest in questions) {
  split_question <- str_split(quest, " ")[[1]]
  
  # Check if the word is 6 letters or greater and that it isn't terms_used
  for (word in split_question) {
    if (!word %in% terms_used & nchar(word) >= 6) {
      terms_used <- c(terms_used, word)
    }
  }
}

```

# Terms in High and Low Value questions

We will now look at terms in both low and high value questions

* Low value are questions worth less than \$800
* High value are questions worth \$800 or greater

We will only use the first 20 terms in the `term_used` vector for speed (as it is `r length(terms_used)` elements)

```{r}

q_values <- pull(jeopardy, value)
q_values_count <- NULL

for (word in terms_used[1:20]) {
  
  n_high_value_q <- 0
  n_low_value_q <- 0
  
  for (q in 1:length(questions)) {
    
    # Split the sentence
    split_question <- str_split(questions[q], " ")[[1]]
    
    # Detect term in question and value status
    if (word %in% split_question & q_values[q] >= 800) {
      n_high_value_q <- n_high_value_q + 1
    } else if (word %in% split_question & q_values[q] < 800) {
      n_low_value_q <- n_low_value_q + 1
    }
    
  }
  
}

```

```{r}
# run the chisquared test on these and add them to the new dataframe
test_value <- chisq.test(c(n_high_value_q, n_low_value_q), p = c(2/5, 3/5))
value_row <- c(word, n_high_value_q, n_low_value_q, test_value$p.value)

# Append it to our new dataframe
q_values_count <- rbind(q_values_count, value_row)
```

Let's look at the dataframe that we created
```{r}
q_values_count
```

Let's put this in a better format
```{r}
clean_q_values_count <- as_tibble(q_values_count)
colnames(clean_q_values_count) <- c("word", "n_high_value", "n_low_value", "p_value")
head(clean_q_values_count)
```

This means that in the 20 terms that we looked at "burger" showed up more in low value questions then high value questions.
